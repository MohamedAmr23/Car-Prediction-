# -*- coding: utf-8 -*-
"""carPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i3rLhfit4Yljp9OuehRVVv2nQqOBsk_D
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import seaborn as sns
sns.set()
import tkinter as tk
from tkinter import ttk
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
data=pd.read_csv("data.csv")

# Create GUI window
window = tk.Tk()
window.title("Car Price Prediction")

data.columns=data.columns.str.lower().str.replace(' ','_')

stringColumns=list(data.dtypes[data.dtypes=='object'].index)

for col in stringColumns:
  data[col]=data[col].str.lower().str.replace(' ','_')

#change column name msrp to price
data.rename(columns = {'msrp': 'price'}, inplace = True)

#2 floating point after (,)
pd.options.display.float_format='{:,.2f}'.format

data['log_price'] = np.log1p(data.price)
np.random.seed(2)     # Fixes the random seed to make sure that the results are reproducible

n = len(data)

n_val = int(0.2 * n)
n_test = int(0.2 * n)
n_train = n - (n_val + n_test)

# print('No. of rows for training : ', n_train)
# print('No. of rows for validation : ', n_val)
# print('No. of rows for testing : ', n_test)

idx = np.arange(n)
# print(idx)
np.random.shuffle(idx)
# print(idx)

df_shuffled = data.iloc[idx]
# print(data.index)
# print(df_shuffled.index)

df_shuffled

data

df_train = df_shuffled.iloc[:n_train].copy()
df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
df_test = df_shuffled.iloc[n_train+n_val:].copy()

df_train.shape

df_val.shape

df_test.shape

y_train = df_train.log_price.values
y_val = df_val.log_price.values
y_test = df_test.log_price.values

# #####################################
base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']   # Think about Numerical only

# data[base]

# data[base].isnull().sum()

#function to Handling Missing Values
def prepare_X(data):
    data_num = data[base]
    data_num = data_num.fillna(data_num.mean())
    X = data_num.values
    return X

def linear_regression(xi):
    n =len(xi)                # Number of features used

    pred = w0                 # Initial / Base prediction

    for j in range(n):
        pred += w[j]*xi[j]     # Formula = w0 +sigma[0:n-1]{w[j]*xi[j]}

    return pred

def train_linear_regression(X, y):
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    XTX = X.T.dot(X)
    XTX_inv = np.linalg.inv(XTX)
    w = XTX_inv.dot(X.T).dot(y)

    return w[0], w[1:]

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)
y_pred = w_0 + X_train.dot(w)

# sns.histplot(y_train, label='target')
# sns.histplot(y_pred, label='prediction', color='red')

# plt.legend()

# plt.ylabel('Frequency')
# plt.xlabel('Log(Price + 1)')
# plt.title('Predictions vs actual distribution')

# plt.show()

def rmse(y, y_pred):
    error = y_pred - y
    mse = (error ** 2).mean()
    return np.sqrt(mse)

rmse(y_train, y_pred)

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)

rmse(y_val, y_pred)

#feature Engineering
sorted(data.year.unique())

def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year    # Because the dataset was created in 2017 (which we can verify by checking df_train.year.max())
    features.append('age')

    df_num = df[features]
    df_num = df_num.fillna(df_num.mean())
    X = df_num.values
    return X

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)
y_pred = w_0 + X_train.dot(w)
# print('Train RMSE: ', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('Validation RMSE: ', rmse(y_val, y_pred))

#Handling Categorical Variables(oneHotEncoding)
data.number_of_doors.value_counts()

data['make'].value_counts().head(10)

def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        df[feature] = (df['number_of_doors'] == v).astype(int)
        features.append(feature)

    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    df_num = df[features]
    df_num = df_num.fillna(df_num.mean())
    X = df_num.values
    return X

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)

y_pred = w_0 + X_train.dot(w)
# print('train:', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('validation:', rmse(y_val, y_pred))

data['engine_fuel_type'].value_counts()

def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        df[feature] = (df['number_of_doors'] == v).astype(int)
        features.append(feature)

    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    for v in ['regular_unleaded', 'premium_unleaded_(required)',
              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
        feature = 'is_type_%s' % v
        df[feature] = (df['engine_fuel_type'] == v).astype(int)
        features.append(feature)

    df_num = df[features]
    df_num = df_num.fillna(0)
    X = df_num.values
    return X

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)

y_pred = w_0 + X_train.dot(w)
# print('train:', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('validation:', rmse(y_val, y_pred))

data['transmission_type'].value_counts()

data['driven_wheels'].value_counts()

data['market_category'].value_counts().head(5)

data['vehicle_size'].value_counts().head(5)

data['vehicle_style'].value_counts().head(5)

def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        df[feature] = (df['number_of_doors'] == v).astype(int)
        features.append(feature)

    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    for v in ['regular_unleaded', 'premium_unleaded_(required)',
              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
        feature = 'is_type_%s' % v
        df[feature] = (df['engine_fuel_type'] == v).astype(int)
        features.append(feature)

    for v in ['automatic', 'manual', 'automated_manual']:
        feature = 'is_transmission_%s' % v
        df[feature] = (df['transmission_type'] == v).astype(int)
        features.append(feature)

    df_num = df[features]
    df_num = df_num.fillna(df_num.mean())
    X = df_num.values
    return X

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)

y_pred = w_0 + X_train.dot(w)
# print('train:', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('validation:', rmse(y_val, y_pred))

def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        df[feature] = (df['number_of_doors'] == v).astype(int)
        features.append(feature)

    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    for v in ['regular_unleaded', 'premium_unleaded_(required)',
              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
        feature = 'is_type_%s' % v
        df[feature] = (df['engine_fuel_type'] == v).astype(int)
        features.append(feature)

    for v in ['automatic', 'manual', 'automated_manual']:
        feature = 'is_transmission_%s' % v
        df[feature] = (df['transmission_type'] == v).astype(int)
        features.append(feature)

    for v in ['front_wheel_drive', 'rear_wheel_drive', 'all_wheel_drive', 'four_wheel_drive']:
        feature = 'is_driven_wheels_%s' % v
        df[feature] = (df['driven_wheels'] == v).astype(int)
        features.append(feature)

    for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']:
        feature = 'is_mc_%s' % v
        df[feature] = (df['market_category'] == v).astype(int)
        features.append(feature)

    for v in ['compact', 'midsize', 'large']:
        feature = 'is_size_%s' % v
        df[feature] = (df['vehicle_size'] == v).astype(int)
        features.append(feature)

    for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']:
        feature = 'is_style_%s' % v
        df[feature] = (df['vehicle_style'] == v).astype(int)
        features.append(feature)

    df_num = df[features]
    df_num = df_num.fillna(df_num.mean())
    X = df_num.values
    return X

X_train = prepare_X(df_train)
w_0, w = train_linear_regression(X_train, y_train)

y_pred = w_0 + X_train.dot(w)
# print('train:', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('validation:', rmse(y_val, y_pred))

w.astype(int) #linear combination search about singular maxtrix

#Regularization
def train_linear_regression_reg(X, y, r=0.0):
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    XTX = X.T.dot(X)
    reg = r * np.eye(XTX.shape[0])
    XTX = XTX + reg

    XTX_inv = np.linalg.inv(XTX)
    w = XTX_inv.dot(X.T).dot(y)

    return w[0], w[1:]

X_train = prepare_X(df_train)

for r in [0, 0.001, 0.01, 0.1, 1, 10]:
    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)
    # print('%5s, %.2f, %.2f, %.2f' % (r, w_0, w[13], w[21]))

X_train = prepare_X(df_train)
w_0, w = train_linear_regression_reg(X_train, y_train, r=0)

y_pred = w_0 + X_train.dot(w)
# print('train', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('val', rmse(y_val, y_pred))

X_train = prepare_X(df_train)
w_0, w = train_linear_regression_reg(X_train, y_train, r=0.01)

y_pred = w_0 + X_train.dot(w)
# print('train', rmse(y_train, y_pred))

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('val', rmse(y_val, y_pred))

X_train = prepare_X(df_train)
X_val = prepare_X(df_val)

for r in [0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:
    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)
    y_pred = w_0 + X_val.dot(w)
    # print('%6s' %r, rmse(y_val, y_pred))

X_train = prepare_X(df_train)
w_0, w = train_linear_regression_reg(X_train, y_train, r=0.01)

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
# print('validation:', rmse(y_val, y_pred))

X_test = prepare_X(df_test)
y_pred = w_0 + X_test.dot(w)
# print('test:', rmse(y_test, y_pred))

def KNN():
    # KNN model with 5 neighbors
    knn = KNeighborsRegressor(n_neighbors=5)

    # Train the KNN model
    knn.fit(X_train, y_train)

    # Predict prices using KNN
    y_pred_knn = knn.predict(X_val)

    # Calculate RMSE for KNN
    rmse_knn = rmse(y_val, y_pred_knn)
    print('KNN validation RMSE:', rmse_knn)

def Linear():
    i = 2
    ad = df_test.iloc[i].to_dict()
    ad

    X_test = prepare_X(pd.DataFrame([ad]))
    y_pred = w_0 + X_test.dot(w)
    suggestion = np.expm1(y_pred)
    suggestion
    print('Linear Regression validation RMSE:', rmse(y_val, y_pred))

def DT():
    # Create a Decision Tree regressor
    dt = DecisionTreeRegressor()

    # Train the Decision Tree model
    dt.fit(X_train, y_train)

    # Predict prices using Decision Tree
    y_pred_dt = dt.predict(X_val)

    # Calculate RMSE for Decision Tree
    rmse_dt = rmse(y_val, y_pred_dt)
    print('Decision Tree validation RMSE:', rmse_dt)

def RF():
   # Create a Random Forest regressor
    rf = RandomForestRegressor()

    # Train the Random Forest model
    rf.fit(X_train, y_train)

    # Predict prices using Random Forest
    y_pred_rf = rf.predict(X_val)

    # Calculate RMSE for Random Forest
    rmse_rf = rmse(y_val, y_pred_rf)
    print('Random Forest validation RMSE:', rmse_rf)

def SVM():
    # Create an SVM regressor
    svm = SVR()

    # Train the SVM model
    svm.fit(X_train, y_train)

    # Predict prices using SVM
    y_pred_svm = svm.predict(X_val)

    # Calculate RMSE for SVM
    rmse_svm = rmse(y_val, y_pred_svm)
    print('SVM validation RMSE:', rmse_svm)

def compareAlgorithms():
    # KNN model with 5 neighbors
    knn = KNeighborsRegressor(n_neighbors=5)

    # Train the KNN model
    knn.fit(X_train, y_train)

    # Predict prices using KNN
    y_pred_knn = knn.predict(X_val)

    # Calculate RMSE for KNN
    rmse_knn = rmse(y_val, y_pred_knn)
    i = 2
    ad = df_test.iloc[i].to_dict()
    ad

    X_test = prepare_X(pd.DataFrame([ad]))
    y_pred = w_0 + X_test.dot(w)
    suggestion = np.expm1(y_pred)
    suggestion

    # Create a Decision Tree regressor
    dt = DecisionTreeRegressor()

    # Train the Decision Tree model
    dt.fit(X_train, y_train)

    # Predict prices using Decision Tree
    y_pred_dt = dt.predict(X_val)

    # Calculate RMSE for Decision Tree
    rmse_dt = rmse(y_val, y_pred_dt)

    # Create a Random Forest regressor
    rf = RandomForestRegressor()

    # Train the Random Forest model
    rf.fit(X_train, y_train)

    # Predict prices using Random Forest
    y_pred_rf = rf.predict(X_val)

    # Calculate RMSE for Random Forest
    rmse_rf = rmse(y_val, y_pred_rf)

     # Create an SVM regressor
    svm = SVR()

    # Train the SVM model
    svm.fit(X_train, y_train)

    # Predict prices using SVM
    y_pred_svm = svm.predict(X_val)

    # Calculate RMSE for SVM
    rmse_svm = rmse(y_val, y_pred_svm)
    print('Decision Tree validation RMSE:', rmse_dt)
    print('Linear Regression validation RMSE:', rmse(y_val, y_pred))
    print('KNN validation RMSE:', rmse(y_val, y_pred_knn))
    print('Decision Tree validation RMSE:', rmse_dt)
    print('Random Forest validation RMSE:', rmse_rf)
    print('SVM validation RMSE:', rmse_svm)



knn = ttk.Button(window, text="Linear Regression validation RMSE", command=Linear)
knn.grid(row=1, column=0,padx=(10)  ,pady=10)

linear = ttk.Button(window, text="KNN validation RMSE", command=KNN)
linear.grid(row=1, column=1,padx=(0,10) , pady=10)

Decision_Tree = ttk.Button(window, text="Decision Tree", command=DT)
Decision_Tree.grid(row=1, column=2, padx=(0,10) , pady=10)

Random_Forest = ttk.Button(window, text="Random Forest", command=RF)
Random_Forest.grid(row=1, column=3,  padx=(0,10) ,pady=10)

svm = ttk.Button(window, text="SVM", command=SVM)
svm.grid(row=1, column=4,  padx=(0,10) ,pady=10)

compareBetweenAlgorithms = ttk.Button(window, text="RMSE For Each Algorithms", command=compareAlgorithms)
compareBetweenAlgorithms.grid(row=1, column=5,  padx=(0,10) ,pady=10)




# Start GUI event loop
window.mainloop()